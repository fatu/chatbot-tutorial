{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk  # For tokenize\n",
    "from tqdm import tqdm  # Progress bar\n",
    "import pickle  # Saving the data\n",
    "import os\n",
    "import random\n",
    "from opensubsdata import OpensubsData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "\n",
    "corpus = 'opensubs'\n",
    "corpus_dir = '/Users/fatu/dev/github/fatu/chatbot-tutorial/data/opensubs'\n",
    "\n",
    "word2id = {}\n",
    "id2word = {}\n",
    "idCount = {}\n",
    "embedding_size = 64\n",
    "softmaxSamples = 0\n",
    "\n",
    "pad_token = -1  # Padding\n",
    "go_token = -1  # Start of sequence\n",
    "eos_token = -1  # End of sequence\n",
    "unknown_token = -1  # Word dropped from vocabulary\n",
    "\n",
    "training_samples = []\n",
    "\n",
    "\n",
    "vocabulary_size = len(word2id)\n",
    "\n",
    "\n",
    "learning_rate = 0.002\n",
    "\n",
    "full_sample_path = '/Users/fatu/dev/github/fatu/chatbot-tutorial/data/samples/dataset-opensubs.pkl'\n",
    "summary_name = '/Users/fatu/dev/github/fatu/chatbot-tutorial/model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWordId(word, create=True):\n",
    "    \"\"\"Get the id of the word (and add it to the dictionary if not existing). If the word does not exist and\n",
    "    create is set to False, the function will return the unknownToken value\n",
    "    Args:\n",
    "        word (str): word to add\n",
    "        create (Bool): if True and the word does not exist already, the world will be added\n",
    "    Return:\n",
    "        int: the id of the word created\n",
    "    \"\"\"\n",
    "    # Should we Keep only words with more than one occurrence ?\n",
    "\n",
    "    word = word.lower()  # Ignore case\n",
    "\n",
    "    # At inference, we simply look up for the word\n",
    "    if not create:\n",
    "        wordId = word2id.get(word, unknown_token)\n",
    "    # Get the id if the word already exist\n",
    "    elif word in word2id:\n",
    "        wordId = word2id[word]\n",
    "        idCount[wordId] += 1\n",
    "    # If not, we create a new entry\n",
    "    else:\n",
    "        wordId = len(word2id)\n",
    "        word2id[word] = wordId\n",
    "        id2word[wordId] = word\n",
    "        idCount[wordId] = 1\n",
    "\n",
    "    return wordId\n",
    "\n",
    "def extractText(line):\n",
    "    \"\"\"Extract the words from a sample lines\n",
    "    Args:\n",
    "        line (str): a line containing the text to extract\n",
    "    Return:\n",
    "        list<list<int>>: the list of sentences of word ids of the sentence\n",
    "    \"\"\"\n",
    "    sentences = []  # List[List[str]]\n",
    "\n",
    "    # Extract sentences\n",
    "    sentencesToken = nltk.sent_tokenize(line)\n",
    "\n",
    "    # We add sentence by sentence until we reach the maximum length\n",
    "    for i in range(len(sentencesToken)):\n",
    "        tokens = nltk.word_tokenize(sentencesToken[i])\n",
    "\n",
    "        tempWords = []\n",
    "        for token in tokens:\n",
    "            tempWords.append(getWordId(token))  # Create the vocabulary and the training sentences\n",
    "\n",
    "        sentences.append(tempWords)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def saveDataset(filename):\n",
    "    \"\"\"Save samples to file\n",
    "    Args:\n",
    "        filename (str): pickle filename\n",
    "    \"\"\"\n",
    "\n",
    "    with open(os.path.join(filename), 'wb') as handle:\n",
    "        data = {  # Warning: If adding something here, also modifying loadDataset\n",
    "            'word2id': word2id,\n",
    "            'id2word': id2word,\n",
    "            'idCount': idCount,\n",
    "            'training_samples': training_samples\n",
    "        }\n",
    "        pickle.dump(data, handle, -1)  # Using the highest protocol available\n",
    "\n",
    "def loadDataset(filename):\n",
    "    \"\"\"Load samples from file\n",
    "    Args:\n",
    "        filename (str): pickle filename\n",
    "    \"\"\"\n",
    "    dataset_path = os.path.join(filename)\n",
    "    print('Loading dataset from {}'.format(dataset_path))\n",
    "    with open(dataset_path, 'rb') as handle:\n",
    "        data = pickle.load(handle)  # Warning: If adding something here, also modifying saveDataset\n",
    "        word2id = data['word2id']\n",
    "        id2word = data['id2word']\n",
    "        idCount = data.get('idCount', None)\n",
    "        training_samples = data['training_samples']\n",
    "\n",
    "        padToken = word2id['<pad>']\n",
    "        goToken = word2id['<go>']\n",
    "        eosToken = word2id['<eos>']\n",
    "        unknownToken = word2id['<unknown>']  # Restore special words\n",
    "    return word2id, id2word, idCount, training_samples, padToken, goToken, eosToken, unknownToken\n",
    "        \n",
    "def tqdm_wrap(iterable, *args, **kwargs):\n",
    "    \"\"\"Forward an iterable eventually wrapped around a tqdm decorator\n",
    "    The iterable is only wrapped if the iterable contains enough elements\n",
    "    Args:\n",
    "        iterable (list): An iterable object which define the __len__ method\n",
    "        *args, **kwargs: the tqdm parameters\n",
    "    Return:\n",
    "        iter: The iterable eventually decorated\n",
    "    \"\"\"\n",
    "    if len(iterable) > 100:\n",
    "        return tqdm(iterable, *args, **kwargs)\n",
    "    return iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load  corpus data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing dataset...\n",
      "Loading dataset from /Users/fatu/dev/github/fatu/chatbot-tutorial/data/samples/dataset-opensubs.pkl\n",
      "Loaded opensubs: 115763 words, 1618483 QA\n"
     ]
    }
   ],
   "source": [
    "print('Constructing dataset...')\n",
    "optional = ''\n",
    "\n",
    "datasetExist = os.path.isfile(full_sample_path)\n",
    "if not datasetExist:\n",
    "    # Corpus creation\n",
    "    corpusData = OpensubsData(corpus_dir)\n",
    "    # createFullCorpus(corpusData.getConversations())\n",
    "    padToken = getWordId('<pad>')  # Padding (Warning: first things to add > id=0 !!)\n",
    "    goToken = getWordId('<go>')  # Start of sequence\n",
    "    eosToken = getWordId('<eos>')  # End of sequence\n",
    "    unknownToken = getWordId('<unknown>')  # Word dropped from vocabulary\n",
    "\n",
    "    # Preprocessing data\n",
    "\n",
    "    for conversation in tqdm(corpusData.getConversations(), desc='Extract conversations'):\n",
    "        \"\"\"Extract the sample lines from the conversations\n",
    "        Args:\n",
    "            conversation (Obj): a conversation object containing the lines to extract\n",
    "        \"\"\"\n",
    "        step = 1\n",
    "        # Iterate over all the lines of the conversation\n",
    "        for i in tqdm_wrap(\n",
    "            range(0, len(conversation['lines']) - 1, step),  # We ignore the last line (no answer for it)\n",
    "            desc='Conversation',\n",
    "            leave=False\n",
    "        ):\n",
    "            inputLine  = conversation['lines'][i]\n",
    "            targetLine = conversation['lines'][i+1]\n",
    "\n",
    "            inputWords  = extractText(inputLine['text'])\n",
    "            targetWords = extractText(targetLine['text'])\n",
    "\n",
    "            if inputWords and targetWords:  # Filter wrong samples (if one of the list is empty)\n",
    "                training_samples.append([inputWords, targetWords])\n",
    "    \n",
    "    print('Filtering words (vocabSize = {} and wordCount > {})...'.format(\n",
    "    vocabulary_size,\n",
    "    1 #filterVocab\n",
    "    ))\n",
    "    \n",
    "    saveDataset(full_sample_path)\n",
    "else:\n",
    "    word2id, id2word, idCount, training_samples, pad_token, go_token, eos_token, unknown_token = loadDataset(full_sample_path)\n",
    "\n",
    "print('Loaded {}: {} words, {} QA'.format(corpus, len(word2id), len(training_samples)))\n",
    "\n",
    "\n",
    "# filterFromFull()  # Extract the sub vocabulary for the given maxLength and filterVocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1618483\n"
     ]
    }
   ],
   "source": [
    "# with tf.device(self.getDevice()):\n",
    "#     model = Model(self.args, self.textData)\n",
    "print(len(training_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create RNN cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ProjectionOp:\n",
    "    \"\"\" Single layer perceptron\n",
    "    Project input tensor on the output dimension\n",
    "    \"\"\"\n",
    "    def __init__(self, shape, scope=None, dtype=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            shape: a tuple (input dim, output dim)\n",
    "            scope (str): encapsulate variables\n",
    "            dtype: the weights type\n",
    "        \"\"\"\n",
    "        assert len(shape) == 2\n",
    "\n",
    "        self.scope = scope\n",
    "\n",
    "        # Projection on the keyboard\n",
    "        with tf.variable_scope('weights_' + self.scope):\n",
    "            self.W_t = tf.get_variable(\n",
    "                'weights',\n",
    "                shape,\n",
    "                # initializer=tf.truncated_normal_initializer()  # TODO: Tune value (fct of input size: 1/sqrt(input_dim))\n",
    "                dtype=dtype\n",
    "            )\n",
    "            self.b = tf.get_variable(\n",
    "                'bias',\n",
    "                shape[0],\n",
    "                initializer=tf.constant_initializer(),\n",
    "                dtype=dtype\n",
    "            )\n",
    "            self.W = tf.transpose(self.W_t)\n",
    "\n",
    "    def getWeights(self):\n",
    "        \"\"\" Convenience method for some tf arguments\n",
    "        \"\"\"\n",
    "        return self.W, self.b\n",
    "\n",
    "    def __call__(self, X):\n",
    "        \"\"\" Project the output of the decoder into the vocabulary space\n",
    "        Args:\n",
    "            X (tf.Tensor): input value\n",
    "        \"\"\"\n",
    "        with tf.name_scope(self.scope):\n",
    "            return tf.matmul(X, self.W) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "units = 512\n",
    "numLayers = 2\n",
    "dropout = 0.9\n",
    "max_length_Deco = 12\n",
    "max_length_enco = 10\n",
    "\n",
    "# Placeholders\n",
    "encoder_inputs  = None\n",
    "decoder_inputs  = None  # Same that decoderTarget plus the <go>\n",
    "decoder_targets = None\n",
    "decoder_weights = None  # Adjust the learning to the target sentence size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl import MultiRNNCell\n",
    "def create_rnn_cell():\n",
    "    enco_deco_cell = tf.contrib.rnn.BasicLSTMCell(units)\n",
    "    enco_deco_cell = tf.contrib.rnn.DropoutWrapper(\n",
    "        enco_deco_cell,\n",
    "        input_keep_prob=1.0,\n",
    "        output_keep_prob=dropout\n",
    "    )\n",
    "    return enco_deco_cell\n",
    "enco_deco_cell = tf.contrib.rnn.MultiRNNCell([create_rnn_cell() for _ in range(numLayers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('placeholder_encoder'):\n",
    "    encoder_inputs  = [tf.placeholder(tf.int32,   [None, ]) for _ in range(max_length_enco)]\n",
    "\n",
    "with tf.name_scope('placeholder_decoder'):\n",
    "    decoder_inputs  = [tf.placeholder(tf.int32,   [None, ], name='inputs') for _ in range(max_length_Deco)]\n",
    "    decoder_targets = [tf.placeholder(tf.int32,   [None, ], name='targets') for _ in range(max_length_Deco)]\n",
    "    decoder_weights = [tf.placeholder(tf.float32, [None, ], name='weights') for _ in range(max_length_Deco)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'placeholder_encoder/Placeholder:0' shape=(?,) dtype=int32>, <tf.Tensor 'placeholder_encoder/Placeholder_1:0' shape=(?,) dtype=int32>, <tf.Tensor 'placeholder_encoder/Placeholder_2:0' shape=(?,) dtype=int32>, <tf.Tensor 'placeholder_encoder/Placeholder_3:0' shape=(?,) dtype=int32>, <tf.Tensor 'placeholder_encoder/Placeholder_4:0' shape=(?,) dtype=int32>, <tf.Tensor 'placeholder_encoder/Placeholder_5:0' shape=(?,) dtype=int32>, <tf.Tensor 'placeholder_encoder/Placeholder_6:0' shape=(?,) dtype=int32>, <tf.Tensor 'placeholder_encoder/Placeholder_7:0' shape=(?,) dtype=int32>, <tf.Tensor 'placeholder_encoder/Placeholder_8:0' shape=(?,) dtype=int32>, <tf.Tensor 'placeholder_encoder/Placeholder_9:0' shape=(?,) dtype=int32>]\n",
      "64\n",
      "<tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.MultiRNNCell object at 0x163670d50>\n",
      "(LSTMStateTuple(c=512, h=512), LSTMStateTuple(c=512, h=512))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shapes (512, 0) and [None, 40004] are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-7197d4865dcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0membedding_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0moutput_projection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputProjection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetWeights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutputProjection\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mfeed_previous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         )\n",
      "\u001b[0;32m/Users/fatu/anaconda/envs/tensorflowpy2/lib/python2.7/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.pyc\u001b[0m in \u001b[0;36membedding_rnn_seq2seq\u001b[0;34m(encoder_inputs, decoder_inputs, cell, num_encoder_symbols, num_decoder_symbols, embedding_size, output_projection, feed_previous, dtype, scope)\u001b[0m\n\u001b[1;32m    376\u001b[0m           \u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m           \u001b[0moutput_projection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_projection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m           feed_previous=feed_previous)\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# If feed_previous is a Tensor, we construct 2 graphs and use cond.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fatu/anaconda/envs/tensorflowpy2/lib/python2.7/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.pyc\u001b[0m in \u001b[0;36membedding_rnn_decoder\u001b[0;34m(decoder_inputs, initial_state, cell, num_symbols, embedding_size, output_projection, feed_previous, update_embedding_for_previous, scope)\u001b[0m\n\u001b[1;32m    284\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m       \u001b[0mproj_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_projection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m       \u001b[0mproj_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_symbols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m       \u001b[0mproj_biases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_projection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m       \u001b[0mproj_biases\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_symbols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fatu/anaconda/envs/tensorflowpy2/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.pyc\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    733\u001b[0m     \"\"\"\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mis_fully_defined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes (512, 0) and [None, 40004] are incompatible"
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "#if 0 < self.args.softmaxSamples < vocabulary_size:\n",
    "dtype = tf.float32\n",
    "\n",
    "outputProjection = ProjectionOp(\n",
    "    (vocabulary_size, units), scope='softmax_projection', dtype=dtype\n",
    ")\n",
    "###\n",
    "print(encoder_inputs)\n",
    "print(embedding_size)\n",
    "print(enco_deco_cell)\n",
    "print(enco_deco_cell.state_size)\n",
    "###\n",
    "\n",
    "decoderOutputs, states = tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq(\n",
    "            encoder_inputs,\n",
    "            decoder_inputs,\n",
    "            enco_deco_cell,\n",
    "            40004, #len(word2id),\n",
    "            40004, #len(word2id),\n",
    "            embedding_size=embedding_size, \n",
    "            output_projection=outputProjection.getWeights() if outputProjection else None,\n",
    "            feed_previous=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-8-f32f56a2a3b0>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-f32f56a2a3b0>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    tf.summary.scalar('loss', self.lossFct)  # Keep track of the cost\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "loss_function = tf.contrib.legacy_seq2seq.sequence_loss(\n",
    "                decoderOutputs,\n",
    "                decoder_targets,\n",
    "                decoder_weights,\n",
    "                vocabulary_size,\n",
    "                softmax_loss_function= sampled_softmax if outputProjection else None  # If None, use default SoftMax\n",
    "            )\n",
    "            tf.summary.scalar('loss', loss_function)  # Keep track of the cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learning_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a4c01e5a172c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize the optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m opt = tf.train.AdamOptimizer(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mbeta1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbeta2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.999\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'learning_rate' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize the optimizer\n",
    "opt = tf.train.AdamOptimizer(\n",
    "    learning_rate=learning_rate,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    epsilon=1e-08\n",
    ")\n",
    "opt_op = opt.minimize(loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saver/summaries\n",
    "writer = tf.summary.FileWriter(summary_name\n",
    "saver = tf.train.Saver(max_to_keep=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflowpy2]",
   "language": "python",
   "name": "conda-env-tensorflowpy2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
